{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBK06QkkTPVsyXalhEjcV6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaharMalaika/CODING-SAMURAI-INTERNSHIP-TASK/blob/main/Chatbot_using_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui1RBbwrlMx0",
        "outputId": "6f8e5ec9-6127-4cf5-fec2-ca11a8d75a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  ##package with pretrained tokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVnpIC1jlTbB",
        "outputId": "05c71249-436a-4d24-fd9d-a5375a650862"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence):\n",
        "  return nltk.word_tokenize(sentence)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "xaaSxrevlfQL"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem(word):\n",
        "  return stemmer.stem(word.lower())"
      ],
      "metadata": {
        "id": "sdY9P6WFlqG_"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def bag_of_words(tokenized_sentence, all_words):\n",
        "  pass\n",
        "\n",
        "  tokenized_sentence = [stem(w) for w in tokenized_sentence]\n",
        "  bag = np.zeros(len(all_words), dtype=np.float32)\n",
        "  for idx, w in enumerate(all_words):\n",
        "    if w in tokenized_sentence:\n",
        "      bag[idx] = 1.0   ##giving 1 to index where the word exists\n",
        "\n",
        "  return bag\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "MYMWduTGlr41"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "import nltk\n",
        "\n",
        "with open('intents.json','r') as f:\n",
        "  intents=json.load(f)\n",
        "\n",
        "\n",
        "all_words=[]\n",
        "tags=[]\n",
        "xy=[]\n",
        "for intent in intents['intents']:\n",
        "  tag=intent['tag']\n",
        "  tags.append(tag)\n",
        "  for pattern in intent['patterns']:\n",
        "    w=tokenize(pattern)\n",
        "    all_words.extend(w)\n",
        "    xy.append((w,tag))\n",
        "  ignore_words=['?',',','.','!']\n",
        "  all_words=[stem(w) for w in all_words if w not in ignore_words]\n",
        "  all_words=sorted(set(all_words))  ##removing duplicate elements\n",
        "  tags=sorted(set(tags))\n",
        "print(tags)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNTWUx0imerm",
        "outputId": "7ad54a8c-736a-4672-9bc6-720eee7fabee"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['delivery', 'funny', 'goodbye', 'greeting', 'items', 'payments', 'thanks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##creating bag of words\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "X_train= []\n",
        "y_train= []\n",
        "for (pattern_sentence,tag) in xy:\n",
        "  bag=bag_of_words(pattern_sentence,all_words)\n",
        "  X_train.append(bag)\n",
        "  label=tags.index(tag)\n",
        "  y_train.append(label) ## cross entropy loss\n",
        "\n",
        "X_train=np.array(X_train)\n",
        "y_train=np.array(y_train)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cgc7c-epnGsX"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.n_samples=len(X_train)\n",
        "    self.x_data=X_train\n",
        "    self.y_data=y_train\n",
        "\n",
        "  #dataset[index]\n",
        "  def __getitem__(self,index):\n",
        "    return self.x_data[index],self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 1000\n",
        "batch_size = 8\n",
        "learning_rate = 0.001\n",
        "input_size = len(X_train[0])\n",
        "hidden_size = 8\n",
        "output_size = len(tags)\n",
        "\n",
        "\n",
        "dataset=ChatDataset()\n",
        "train_loader=DataLoader(dataset=dataset,batch_size=8,shuffle=True,num_workers=0)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kftJkkQ5rwj9"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "#from model import NeuralNet\n",
        "\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.l1 = nn.Linear(input_size, hidden_size)\n",
        "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):   ##bag of words as input\n",
        "        out = self.l1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.l2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.l3(out)\n",
        "        # no activation and no softmax at the end\n",
        "        return out"
      ],
      "metadata": {
        "id": "Xw5WvJLctLN4"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)"
      ],
      "metadata": {
        "id": "CLxh3_gytx_i"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "9UOxzKsLsYne"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    for (words, labels) in train_loader:\n",
        "        words = words.to(device)\n",
        "        labels = labels.to(dtype=torch.long).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(words)\n",
        "        # if y would be one-hot, we must apply\n",
        "        # labels = torch.max(labels, 1)[1]\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "print(f'final loss: {loss.item():.4f}')\n",
        "\n",
        "data = {\n",
        "\"model_state\": model.state_dict(),\n",
        "\"input_size\": input_size,\n",
        "\"hidden_size\": hidden_size,\n",
        "\"output_size\": output_size,\n",
        "\"all_words\": all_words,\n",
        "\"tags\": tags\n",
        "}\n",
        "\n",
        "FILE = \"data.pth\"\n",
        "torch.save(data, FILE)\n",
        "\n",
        "print(f'training complete. file saved to {FILE}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfdX08BFscjl",
        "outputId": "7b10a257-b8eb-490c-cf0e-c5a32adfcdbb"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 1.2844\n",
            "Epoch [200/1000], Loss: 0.0692\n",
            "Epoch [300/1000], Loss: 0.0467\n",
            "Epoch [400/1000], Loss: 0.0076\n",
            "Epoch [500/1000], Loss: 0.0026\n",
            "Epoch [600/1000], Loss: 0.0043\n",
            "Epoch [700/1000], Loss: 0.0010\n",
            "Epoch [800/1000], Loss: 0.0135\n",
            "Epoch [900/1000], Loss: 0.0020\n",
            "Epoch [1000/1000], Loss: 0.0016\n",
            "final loss: 0.0016\n",
            "training complete. file saved to data.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "import torch\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KGQxGT9kwjl7"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "with open('intents.json', 'r') as f:\n",
        "    intents = json.load(f)"
      ],
      "metadata": {
        "id": "M4zAzrnGw0pb"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FILE = \"data.pth\"\n",
        "data = torch.load( FILE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0FgnvNiyi3f",
        "outputId": "97e9de45-f97e-4281-cd92-51e50496ee21"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-a234400205f2>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load( FILE)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size= data[\"input_size\"]\n",
        "hidden_size=data[\"hidden_size\"]\n",
        "output_size=data[\"output_size\"]\n",
        "all_words=data[\"all_words\"]\n",
        "tags=data[\"tags\"]\n",
        "model_state=data[\"model_state\"]"
      ],
      "metadata": {
        "id": "h6uqqSUWyo43"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "model.load_state_dict(model_state)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uebrGVbXy9cf",
        "outputId": "82aa60c6-0747-4bbc-ab2c-96b1d813e9a1"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNet(\n",
              "  (l1): Linear(in_features=54, out_features=8, bias=True)\n",
              "  (l2): Linear(in_features=8, out_features=8, bias=True)\n",
              "  (l3): Linear(in_features=8, out_features=7, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot_name= \"SAM\"\n",
        "print(\"Let's chat! type 'quit' to exit\")\n",
        "while True:\n",
        "  sentence=input(\"You: \")\n",
        "  if sentence==\"quit\":\n",
        "    break\n",
        "\n",
        "  sentence=tokenize(sentence)\n",
        "  X=bag_of_words(sentence,all_words)\n",
        "  X=X.reshape(1,X.shape[0])\n",
        "  X=torch.from_numpy(X)\n",
        "\n",
        "  output=model(X)\n",
        "  _,predicted=torch.max(output,dim=1)\n",
        "  tag=tags[predicted.item()]\n",
        "\n",
        "  probs=torch.softmax(output,dim=1)\n",
        "  prob=probs[0][predicted.item()]\n",
        "\n",
        "  if prob.item()>0.75:\n",
        "    for intent in intents[\"intents\"]:\n",
        "      if tag==intent[\"tag\"]:\n",
        "        print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
        "  else:\n",
        "    print(f\"{bot_name}: I do not understand...\")\n",
        "\n",
        "  for intent in intents[\"intents\"]:\n",
        "    if tag==intent[\"tag\"]:\n",
        "      print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSVgv53VzV3g",
        "outputId": "256ad629-d0c0-4275-ae6c-bd37590b24e6"
      },
      "execution_count": 63,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's chat! type 'quit' to exit\n",
            "You: hi\n",
            "SAM: Hi there, how can I help?\n",
            "SAM: Hey :-)\n",
            "You: what do u sell\n",
            "SAM: We have coffee and tea\n",
            "SAM: We sell coffee and tea\n",
            "You: how long takes shipping\n",
            "SAM: Delivery takes 2-4 days\n",
            "SAM: Shipping takes 2-4 days\n",
            "You: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pD4JrXe70GVh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}